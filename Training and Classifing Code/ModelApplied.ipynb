{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d6c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/nyuad/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nyuad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/nyuad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 603 ms, sys: 142 ms, total: 745 ms\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re as charRemoval\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ebc2405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsPreProcessing:\n",
    "    def TweetsProcessing(self, tweetlists):\n",
    "        Tweets_Processed=[]\n",
    "        for t in tweetlists:\n",
    "            Tweets_Processed.append((self.TweetCleaner(t)))\n",
    "        return Tweets_Processed\n",
    "    # tweet cleaner function convert texts to lowercases and removes hashtags urls and usernames\n",
    "    def TweetCleaner(self, t):\n",
    "        stopwordlist = list(stopwords.words('english'))\n",
    "        urlLinks=r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "        username='@[^\\s]+'\n",
    "        Alpha_Patterns_Rep = \"[^a-zA-Z0-9]\"\n",
    "        Sequence_Pattern_Rep   = r\"(.)\\1\\1+\"\n",
    "        Sequence_Replace_Pattern = r\"\\1\\1\"\n",
    "        emojis_Replacements = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "                  ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "                  ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "                  ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "                  '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "                  '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "                  ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "        t = t.lower()\n",
    "        t = charRemoval.sub(urlLinks,' URL',t)\n",
    "        for emoji in emojis_Replacements.keys():\n",
    "            t = t.replace(emoji, \"EMOJI\" + emojis_Replacements[emoji]) \n",
    "        t = charRemoval.sub(username, ' USER', t)\n",
    "        t = charRemoval.sub(Alpha_Patterns_Rep, \" \", t)\n",
    "        t = charRemoval.sub(Sequence_Pattern_Rep, Sequence_Replace_Pattern, t)\n",
    "        wordcleaned=' '\n",
    "        #loop through t and if the word is important then append it\n",
    "        for w in t.split():\n",
    "#             if w not in stopwordlist:\n",
    "            if len(w)>1: wordcleaned += (w+' ')\n",
    "#                 w = WordNetLemmatizer().lemmatize(w)      \n",
    "        return wordcleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff68f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    # Load the vectoriser.\n",
    "    file = open('vectoriser.pickle', 'rb')\n",
    "    vectoriser = pickle.load(file)\n",
    "    file.close()\n",
    "    # Load the LR Model.\n",
    "    file = open('LogisticRegressionModel.pickle', 'rb')\n",
    "    LRmodel = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return vectoriser, LRmodel\n",
    "\n",
    "def predict(vectoriser, model, text):\n",
    "    # Predict the sentiment\n",
    "    processedtext =TweetsPreProcessing().TweetsProcessing(text)\n",
    "    textdata = vectoriser.transform(processedtext)\n",
    "    st = model.predict(textdata)\n",
    "    sentiment=[\"Sentiment\"]\n",
    "    processedtext.insert(0, \"Processed Tweets\")\n",
    "    for i in st:\n",
    "        if i==0:\n",
    "            sentiment.append(\"Negative\")\n",
    "        else:\n",
    "            sentiment.append(\"Positive\")\n",
    "    return sentiment, processedtext\n",
    "\n",
    "def loadtext(filename):\n",
    "    data_set = pd.read_csv(filename, names=[\"author id\", \"created_at\", \"geo\", \"id\", \"lang\", \"like_count\", \"quote_count\", \"reply_count\", \"retweet_count\", \"source\", \"tweet\"],  encoding=\"ISO-8859-1\" )\n",
    "    data_set = data_set[[\"author id\", \"created_at\", \"geo\", \"id\", \"lang\", \"like_count\", \"quote_count\", \"reply_count\", \"retweet_count\", \"source\", \"tweet\"]]\n",
    "    f11= list(data_set[\"tweet\"])\n",
    "    f11.pop(0)\n",
    "    return f11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8a54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_col_to_csv(csvfile,fileout,new_list):\n",
    "    with open(csvfile, 'r') as read_f, \\\n",
    "        open(fileout, 'w', newline='') as write_f:\n",
    "        csv_reader = csv.reader(read_f)\n",
    "        csv_writer = csv.writer(write_f)\n",
    "        i = 0\n",
    "        for row in csv_reader:\n",
    "            row.append(new_list[i])\n",
    "            csv_writer.writerow(row)\n",
    "            i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "633fb6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/f14mp2yd7sq7v92vn6ktx97w0000gn/T/ipykernel_52124/312876201.py:6: DtypeWarning: Columns (0,3,5,6,7,8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  text  = loadtext(datainput[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Applying:  Vaccine.csv\n",
      "Done Applying:  Pfizer.csv\n",
      "Done Applying:  moderna.csv\n",
      "Done Applying:  sinopharm.csv\n",
      "Done Applying:  Johnson.csv\n",
      "Done Applying:  Booster.csv\n",
      "Done Applying:  antivax.csv\n"
     ]
    }
   ],
   "source": [
    "datainput=[\"Vaccine.csv\", \"Pfizer.csv\", \"moderna.csv\", \"sinopharm.csv\", \"Johnson.csv\", \"Booster.csv\", \"antivax.csv\"]\n",
    "dataoutput=[\"VaccineSentiment.csv\", \"PfizerSentiment.csv\", \"modernaSentiment.csv\", \"sinopharmSentiment.csv\", \"JohnsonSentiment.csv\", \"BoosterSentiment.csv\", \"antivaxSentiment.csv\"]\n",
    "temp=[\"temp1.csv\",\"temp2.csv\",\"temp3.csv\",\"temp4.csv\",\"temp5.csv\",\"temp6.csv\",\"temp7.csv\"]\n",
    "vectoriser, LRmodel = load_models()\n",
    "for i in range(7):\n",
    "    text  = loadtext(datainput[i])\n",
    "    sentiment, processedtext = predict(vectoriser, LRmodel, text)\n",
    "    add_col_to_csv(datainput[i],temp[i],processedtext)\n",
    "    add_col_to_csv(temp[i],dataoutput[i],sentiment)\n",
    "    print(\"Done Applying: \", datainput[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
